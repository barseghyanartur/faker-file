{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a `DOCX` file with fake content\n",
    "- Generate 1 `DOCX` file with fake content (generated by `Faker`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/tmpuqx1dlvl.docx\n",
      "/tmp/tmp/tmpuqx1dlvl.docx\n",
      "Official remain wear family democratic hair billion. Event money task option kitchen. Office build ground others practice along security. Travel section discuss ago job capital heavy which.\n",
      "Later nation affect almost then like foreign. Training born discover worker individual.\n",
      "Consumer suggest per appear. Enter ahead doctor. Involve travel form purpose part ability bring.\n",
      "Staff hope claim much. Run important state appear occur. Most bill single teach.\n",
      "Paper hand close choose part major statement door. Two least strategy whether reveal concern. Pass series hair.\n",
      "Without campaign strategy direction. Blue organization true data base. Interview forward organization tend wide break central.\n",
      "Save these wife matter. Friend wait sure attack he.\n",
      "Already so agree health today line manager activity. Participant argue billion once serious police it.\n",
      "Various consumer land agency. Marriage economy anything source matter.\n",
      "Yet fill resource far up.\n",
      "Ask animal involve computer court. These form treat better suggest talk popular.\n",
      "Debate challenge land various pretty enter able. Rate drive lawyer system traditional.\n",
      "Person rest long bad. Month wait near north industry boy she.\n",
      "Interesting make your throughout because. Beautiful prove water attorney would leader just. Better response simply front cost they.\n",
      "Amount score about middle life free couple. Gun work knowledge skill laugh. Level few rule religious recently best.\n",
      "Home capital song start mouth gas.\n",
      "Often thus art most receive whatever some. Series anyone performance whose current every west.\n",
      "Case baby especially author.\n",
      "Agreement try cup. Method magazine work garden financial. Agree myself TV whether mention lawyer.\n",
      "Evening improve really hope fill go. Anyone itself mean them simply budget gun. Most today represent direction son finally so.\n",
      "You expert history blood personal measure. Key picture skill indeed process want staff seat. Live good pull project senior.\n",
      "Poor skin consider benefit lead. Money suddenly they avoid.\n",
      "Pressure maybe plan your tonight yet. Good sometimes exist. Go room continue memory over relate factor.\n",
      "Child nation resource interview. Civil claim people attention white because experience itself. Forget huge story term wind. Strong speech consumer region large class.\n",
      "Measure head property method reduce charge. Magazine behind common piece. Likely important candidate fight.\n",
      "Hundred senior close goal. Majority sea and doctor. Read idea network child certain authority third.\n",
      "Occur speech he meeting argue. Public lot but knowledge.\n",
      "Mean cell hard find safe maintain popular director. Threat class surface agency conference whose. Short red particularly card between.\n",
      "North officer smile without. Garden fund cultural form season more.\n",
      "Arm skin future how player step. Garden serve commercial tell free investment wall.\n",
      "Tough bill religious space.\n",
      "Else economic bank particularly how man race. Respond much value chair this. Fight wide suddenly whom someone.\n",
      "Get wait pretty themselves something town usually. Prevent student perform. Itself no culture rest boy type.\n",
      "Create their son her food. Although base fact drop point everybody. His gas crime floor.\n",
      "Nature meet American charge size. Become city accept test decision. Paper program both arm method woman.\n",
      "Paper number apply few. Service stay civil opportunity. Final away dog TV police specific assume.\n",
      "North poor agent fish want court.\n",
      "Evidence raise catch happen beyond born if. Use car degree brother. Detail own buy rich field city everything.\n",
      "Century pick large international concern writer. Rate where yard beyond perform grow necessary.\n",
      "Situation social choose under leave by. Race make none.\n",
      "May on act experience. Theory policy response first throw dark.\n",
      "Face eat behavior these democratic. He consumer room blue guess professor social. Bit short purpose factor.\n",
      "Author type reduce artist billion picture rock. Physical trade head sister. Tonight investment because. Language gun end hair live skill.\n",
      "Area culture sense people cause size everyone huge. Paper physical whom imagine late.\n",
      "Eat move risk resource former religious movement. Election whom show decade finish mind once military. Cultural on station seven one performance. North most article election too pick.\n",
      "Reach school whom training. Third one door if tough have business. Community military again manager strong.\n",
      "Six page factor pick plan. Offer involve environmental. Later what available we behavior.\n",
      "Form recognize wrong such. Bank old pay fear.\n",
      "Consider though pretty two. Probably federal enjoy institution. Effect hot experience.\n",
      "They become few give society. Concern price sure skin possible. Rather she source fine area.\n",
      "See continue security among argue. Spring common rate perhaps. Like present activity people campaign look least pass.\n",
      "Degree movie process meet. Tell population knowledge write pretty market position. Major paper dog else.\n",
      "Find most across. Treatment follow stay store again blue.\n",
      "So with push than particularly. Sell three job surface way. Catch half peace great. Manage indicate near throw reality health toward.\n",
      "Mouth mean democratic leader left ability pass. Try magazine Mrs plan top. Mention establish do southern soon line way. Court still question tough.\n",
      "Raise democratic community job. Catch defense wall sell. Special me condition real loss financial talk.\n",
      "Ahead nothing usually network marriage occur contain federal. Glass theory various evidence three road. Expert degree list suddenly hour.\n",
      "Shake firm sister especially figure perform. Determine stuff authority off responsibility. Manager rise here bill.\n",
      "Box now kid without for water. People experience may wait campaign candidate around baby.\n",
      "Yet finish sing magazine military cup style last. Throw opportunity start last without. Age your say.\n",
      "Test consumer never game attack would four. Husband can interview while claim cost.\n",
      "Word win defense trial others hour page. Top owner though example whole candidate apply. Process strategy memory theory ever.\n",
      "Body oil college coach none fight indicate. Drop understand hotel argue hit practice rich.\n",
      "Somebody buy theory arrive cut space stop. Prove send guy front beat himself something.\n",
      "Member upon away democratic national community particularly. Young already religious go article court participant maintain. Defense want war tell low.\n",
      "Political animal bed red often condition. Foot reduce above begin.\n",
      "Baby us say difference. Turn tend responsibility note. Kitchen nice material per.\n",
      "Yes activity mission general. Money federal available.\n",
      "Federal conference while.\n",
      "Onto him simple. Recently serve drive whole company either. Beat story learn military service idea.\n",
      "Test benefit benefit imagine many. Item attorney however amount can with nation child. Sea before away.\n",
      "Firm thought cup current fact child may. Stand indeed project rather. Nearly through century sing practice future away.\n",
      "Argue relate idea man. Heavy challenge respond foreign.\n",
      "Much job center quite yard first reflect. Perhaps throughout interesting.\n",
      "Short message stand offer. Particularly detail rock feeling within opportunity special. Ten happy forward.\n",
      "Defense summer man do certain. Leader become reduce popular.\n",
      "Billion owner role under. Reflect election time tend affect again. History choose impact increase.\n",
      "Miss front use term where when say left. Former your moment laugh finally according civil.\n",
      "Write bill sure food economic girl. Kind sense kid exist record west.\n",
      "Suffer American south change everything tax always.\n",
      "Now however party. Sell soon growth act themselves society because Republican. Wife left claim job institution read at.\n",
      "Party to discuss hear often recognize region. Any fish skill choose.\n",
      "Audience degree soldier stop choice.\n",
      "Bit material between final floor production. Trouble get down region. Remember growth month just nation vote fund second.\n",
      "Remember water raise military carry. Power wrong pattern know rock own thought. Measure window feeling low tree interest. Occur measure beat remain.\n",
      "Daughter candidate whose listen technology front recognize. Seek hair want environment get phone.\n",
      "Time room season others without realize thought. Long area my individual capital.\n",
      "Learn fund far another whose sport high artist. East note office. Head in top sure together.\n",
      "Away of pretty stay organization crime early. Democrat rock suggest personal list join. Beautiful place a mother.\n",
      "Story somebody detail involve cost fall. Nature not good trial.\n",
      "Child bag skill position treat tree join. What trial base treat himself word fund. Strong base others present career pattern let network. Base heart have rest evening between.\n",
      "Affect soon term really bank fast history court. Purpose management work well player different fine. Cut building send necessary.\n",
      "Close machine important late friend important. Star true interest serve.\n",
      "Reason theory blue base. Traditional military region executive.\n",
      "Responsibility second out cultural. Half arrive others provide fill fine teacher. And spring relate really type change party.\n",
      "Role suggest when option huge painting. Knowledge participant past street skill.\n",
      "Employee tell scientist true. Recent detail true natural. Chair we well military at cultural.\n",
      "Country happy there issue. Any must free throw weight note watch. Suggest certainly should detail sister protect democratic.\n",
      "Leave able actually race. Task production arm movie list need fire.\n",
      "Local radio season so act audience. Congress arrive recently control card owner. Study make strategy manage pressure over.\n",
      "Space PM consumer baby seek mention. Southern skin doctor star peace avoid mind.\n",
      "Husband sea fast democratic piece Republican notice. Foot case course law many. Sit receive state see seem environment.\n",
      "Scene man total feel.\n",
      "Democratic western over strategy. Film base tonight board.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meeting toward month black morning east. Information throw determine measure. Ready glass to reason why economy game individual.\n"
     ]
    }
   ],
   "source": [
    "from faker import Faker\n",
    "from faker_file.providers.docx_file import DocxFileProvider\n",
    "\n",
    "FAKER = Faker()\n",
    "\n",
    "FAKER.add_provider(DocxFileProvider)\n",
    "\n",
    "file = FAKER.docx_file()\n",
    "\n",
    "print(file)\n",
    "print(file.data[\"filename\"])\n",
    "print(file.data[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide content manually\n",
    "- Generate 1 `DOCX` file with developer defined content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/tmpjjesojaw.docx\n",
      "\n",
      "“The Queen of Hearts, she made some tarts,\n",
      "    All on a summer day:\n",
      "The Knave of Hearts, he stole those tarts,\n",
      "    And took them quite away.”\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TEXT = \"\"\"\n",
    "“The Queen of Hearts, she made some tarts,\n",
    "    All on a summer day:\n",
    "The Knave of Hearts, he stole those tarts,\n",
    "    And took them quite away.”\n",
    "\"\"\"\n",
    "\n",
    "file = FAKER.docx_file(content=TEXT)\n",
    "\n",
    "print(file)\n",
    "print(file.data[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similarly, generate 1 `PNG` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/tmpz6ajp2yh.png\n"
     ]
    }
   ],
   "source": [
    "from faker_file.providers.png_file import PngFileProvider\n",
    "\n",
    "FAKER.add_provider(PngFileProvider)\n",
    "\n",
    "file = FAKER.png_file()\n",
    "\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similarly, generate 1 `PDF` file. Limit the line width to 80 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/tmpuhvef6qr.pdf\n"
     ]
    }
   ],
   "source": [
    "from faker_file.providers.pdf_file import PdfFileProvider\n",
    "\n",
    "FAKER.add_provider(PdfFileProvider)\n",
    "\n",
    "file = FAKER.pdf_file(wrap_chars_after=80)\n",
    "\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide templated content\n",
    "\n",
    "You can generate documents from pre-defined templates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/tmpd9m857qp.pdf\n",
      "\n",
      "2012-05-29 Lake Alexton, Anguilla\n",
      "\n",
      "Hello Timothy Francis,\n",
      "\n",
      "Prove break reality\n",
      "answer well. Protect yes who front finish family kitchen. Firm game adult\n",
      "picture adult natural wide no.\n",
      "\n",
      "Address: 97916 Joshua Village Apt. 722\n",
      "Benjamintown, GU 44271\n",
      "\n",
      "Best regards,\n",
      "\n",
      "William Clark\n",
      "1855 Manuel Fords Apt. 716\n",
      "Lake Andre, PR 63793\n",
      "+1-005-969-4858x973\n"
     ]
    }
   ],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "{{date}} {{city}}, {{country}}\n",
    "\n",
    "Hello {{name}},\n",
    "\n",
    "{{text}}\n",
    "\n",
    "Address: {{address}}\n",
    "\n",
    "Best regards,\n",
    "\n",
    "{{name}}\n",
    "{{address}}\n",
    "{{phone_number}}\n",
    "\"\"\"\n",
    "\n",
    "file = FAKER.pdf_file(content=TEMPLATE, wrap_chars_after=80)\n",
    "\n",
    "print(file)\n",
    "print(file.data[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archive types\n",
    "#### ZIP archive containing 5 TXT files\n",
    "As you might have noticed, some archive types are also supported.\n",
    "The created archive will contain 5 files in TXT format (defaults)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/tmpct2tr53x.zip\n",
      "{'inner': {'tmp/tmp0ai3cyzm.txt': 'tmp/tmp0ai3cyzm.txt', 'tmp/tmp50m9g96y.txt': 'tmp/tmp50m9g96y.txt', 'tmp/tmpk6hxw3hu.txt': 'tmp/tmpk6hxw3hu.txt', 'tmp/tmpw3hfd5ih.txt': 'tmp/tmpw3hfd5ih.txt', 'tmp/tmpnkv_6d8x.txt': 'tmp/tmpnkv_6d8x.txt'}, 'files': [PosixPath('tmp0ai3cyzm.txt'), PosixPath('tmp50m9g96y.txt'), PosixPath('tmpk6hxw3hu.txt'), PosixPath('tmpw3hfd5ih.txt'), PosixPath('tmpnkv_6d8x.txt')], 'filename': '/tmp/tmp/tmpct2tr53x.zip'}\n"
     ]
    }
   ],
   "source": [
    "from faker_file.providers.zip_file import ZipFileProvider\n",
    "\n",
    "FAKER.add_provider(ZipFileProvider)\n",
    "\n",
    "file = FAKER.zip_file()\n",
    "\n",
    "print(file)\n",
    "print(file.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ZIP archive containing 3 DOCX files with text generated from a template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/zzzdjvkeb6h.zip\n",
      "{'inner': {'tmp/xxx_qro372jq.docx': 'tmp/xxx_qro372jq.docx', 'tmp/xxx_48g5sanj.docx': 'tmp/xxx_48g5sanj.docx', 'tmp/xxx_u8840_px.docx': 'tmp/xxx_u8840_px.docx'}, 'files': [PosixPath('yyy/xxx_qro372jq.docx'), PosixPath('yyy/xxx_48g5sanj.docx'), PosixPath('yyy/xxx_u8840_px.docx')], 'filename': '/tmp/tmp/zzzdjvkeb6h.zip'}\n"
     ]
    }
   ],
   "source": [
    "from faker_file.providers.helpers.inner import create_inner_docx_file\n",
    "\n",
    "file = FAKER.zip_file(\n",
    "    prefix=\"zzz\",\n",
    "    options={\n",
    "        \"count\": 3,\n",
    "        \"create_inner_file_func\": create_inner_docx_file,\n",
    "        \"create_inner_file_args\": {\n",
    "            \"prefix\": \"xxx_\",\n",
    "            \"content\": TEMPLATE,\n",
    "        },\n",
    "        \"directory\": \"yyy\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(file)\n",
    "print(file.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nested ZIP archive\n",
    "And of course nested archives are supported too. Create a `ZIP` file which\n",
    "contains 5 `ZIP` files which contain 5 `ZIP` files which contain 2 `DOCX`\n",
    "files.\n",
    "\n",
    "- 5 `ZIP` files in the `ZIP` archive.\n",
    "- Content is generated dynamically.\n",
    "- Prefix the filenames in archive with ``nested_level_1_``.\n",
    "- Prefix the filename of the archive itself with ``nested_level_0_``.\n",
    "- Each of the `ZIP` files inside the `ZIP` file in their turn contains 5 other `ZIP`\n",
    "  files, prefixed with ``nested_level_2_``, which in their turn contain 2\n",
    "  DOCX files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/nested_level_0_mk92rvxx.zip\n",
      "{'inner': {'tmp/nested_level_1_hnw1hj90.zip': 'tmp/nested_level_1_hnw1hj90.zip', 'tmp/nested_level_1_byd96t3l.zip': 'tmp/nested_level_1_byd96t3l.zip', 'tmp/nested_level_1_ishzdbt1.zip': 'tmp/nested_level_1_ishzdbt1.zip', 'tmp/nested_level_1_2u_in9xa.zip': 'tmp/nested_level_1_2u_in9xa.zip', 'tmp/nested_level_1_12q466zn.zip': 'tmp/nested_level_1_12q466zn.zip'}, 'files': [PosixPath('nested_level_1_hnw1hj90.zip'), PosixPath('nested_level_1_byd96t3l.zip'), PosixPath('nested_level_1_ishzdbt1.zip'), PosixPath('nested_level_1_2u_in9xa.zip'), PosixPath('nested_level_1_12q466zn.zip')], 'filename': '/tmp/tmp/nested_level_0_mk92rvxx.zip'}\n"
     ]
    }
   ],
   "source": [
    "from faker_file.providers.helpers.inner import create_inner_zip_file\n",
    "\n",
    "file = FAKER.zip_file(\n",
    "    prefix=\"nested_level_0_\",\n",
    "    options={\n",
    "        \"create_inner_file_func\": create_inner_zip_file,\n",
    "        \"create_inner_file_args\": {\n",
    "            \"prefix\": \"nested_level_1_\",\n",
    "            \"options\": {\n",
    "                \"create_inner_file_func\": create_inner_zip_file,\n",
    "                \"create_inner_file_args\": {\n",
    "                    \"prefix\": \"nested_level_2_\",\n",
    "                    \"options\": {\n",
    "                        \"count\": 2,\n",
    "                        \"create_inner_file_func\": create_inner_docx_file,\n",
    "                        \"create_inner_file_args\": {\n",
    "                            \"content\": TEXT + \"\\n\\n{{date}}\",\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "print(file)\n",
    "print(file.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works similarly for `EML` files (using ``EmlFileProvider``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/zzz40v1czd3.eml\n",
      "{'content': '\\n2001-07-28 Lake Gregorytown, Vietnam\\n\\nHello Angela Horne,\\n\\nBuy sure might manage different among position. Culture particular example traditional accept result course claim.\\n\\nAddress: PSC 1563, Box 6796\\nAPO AA 14817\\n\\nBest regards,\\n\\nMark Barnett\\n4268 Michael Field Suite 877\\nSouth Toddchester, AK 97532\\n001-024-037-4662x107\\n', 'inner': {'tmp/xxx_pp4njct9.docx': 'tmp/xxx_pp4njct9.docx', 'tmp/xxx_0ok7hoze.docx': 'tmp/xxx_0ok7hoze.docx', 'tmp/xxx_w_lpa4ay.docx': 'tmp/xxx_w_lpa4ay.docx'}, 'filename': '/tmp/tmp/zzz40v1czd3.eml', 'to': 'claudia26@example.com', 'from': 'ctrevino@example.org', 'subject': 'Glass wish build group task reflect.'}\n"
     ]
    }
   ],
   "source": [
    "from faker_file.providers.eml_file import EmlFileProvider\n",
    "from faker_file.providers.helpers.inner import create_inner_docx_file\n",
    "\n",
    "FAKER.add_provider(EmlFileProvider)\n",
    "\n",
    "file = FAKER.eml_file(\n",
    "    prefix=\"zzz\",\n",
    "    content=TEMPLATE,\n",
    "    options={\n",
    "        \"count\": 3,\n",
    "        \"create_inner_file_func\": create_inner_docx_file,\n",
    "        \"create_inner_file_args\": {\n",
    "            \"prefix\": \"xxx_\",\n",
    "            \"content\": TEXT + \"\\n\\n{{date}}\",\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "print(file)\n",
    "print(file.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a ZIP file with random variety of different file types within\n",
    "- 50 files in the ZIP archive (limited to DOCX, EPUB and TXT types).\n",
    "- Content is generated dynamically.\n",
    "- Prefix the filename of the archive itself with zzz_archive_.\n",
    "- Inside the ZIP, put all files in directory zzz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/zzz_archive__nknjpdp.zip\n",
      "{'inner': {'tmp/tmpzoho_vd7.docx': 'tmp/tmpzoho_vd7.docx', 'tmp/tmp89zqc_jv.epub': 'tmp/tmp89zqc_jv.epub', 'tmp/tmpmt0iwb6m.docx': 'tmp/tmpmt0iwb6m.docx', 'tmp/tmp0sbkc8r2.txt': 'tmp/tmp0sbkc8r2.txt', 'tmp/tmpocglwi1r.docx': 'tmp/tmpocglwi1r.docx', 'tmp/tmpe2lc8mex.epub': 'tmp/tmpe2lc8mex.epub', 'tmp/tmpv5rwd8pz.epub': 'tmp/tmpv5rwd8pz.epub', 'tmp/tmpy7mvapwz.epub': 'tmp/tmpy7mvapwz.epub', 'tmp/tmp2bhcq04q.epub': 'tmp/tmp2bhcq04q.epub', 'tmp/tmpq8h90phv.docx': 'tmp/tmpq8h90phv.docx', 'tmp/tmp7kbo4ecl.epub': 'tmp/tmp7kbo4ecl.epub', 'tmp/tmp76e2l063.epub': 'tmp/tmp76e2l063.epub', 'tmp/tmpxzo6__jy.txt': 'tmp/tmpxzo6__jy.txt', 'tmp/tmpjmkks4l9.docx': 'tmp/tmpjmkks4l9.docx', 'tmp/tmpg3dn3h33.txt': 'tmp/tmpg3dn3h33.txt', 'tmp/tmp5rb23zbf.epub': 'tmp/tmp5rb23zbf.epub', 'tmp/tmpzj9qv0_f.docx': 'tmp/tmpzj9qv0_f.docx', 'tmp/tmplzx7gu69.txt': 'tmp/tmplzx7gu69.txt', 'tmp/tmpl_nqcygs.docx': 'tmp/tmpl_nqcygs.docx', 'tmp/tmpa2lw8d93.txt': 'tmp/tmpa2lw8d93.txt', 'tmp/tmpk1wu8zaf.txt': 'tmp/tmpk1wu8zaf.txt', 'tmp/tmpxg37xiob.epub': 'tmp/tmpxg37xiob.epub', 'tmp/tmpoxpwctgv.docx': 'tmp/tmpoxpwctgv.docx', 'tmp/tmpyvo9auke.docx': 'tmp/tmpyvo9auke.docx', 'tmp/tmpppe5qssx.epub': 'tmp/tmpppe5qssx.epub', 'tmp/tmpjkn4zd8j.txt': 'tmp/tmpjkn4zd8j.txt', 'tmp/tmplu9crnpu.txt': 'tmp/tmplu9crnpu.txt', 'tmp/tmpbkae0n_8.epub': 'tmp/tmpbkae0n_8.epub', 'tmp/tmpf6e4h0ni.epub': 'tmp/tmpf6e4h0ni.epub', 'tmp/tmpyhxqp5nv.docx': 'tmp/tmpyhxqp5nv.docx', 'tmp/tmptmk9fpto.epub': 'tmp/tmptmk9fpto.epub', 'tmp/tmp2wb2r610.txt': 'tmp/tmp2wb2r610.txt', 'tmp/tmpzb3i8a84.docx': 'tmp/tmpzb3i8a84.docx', 'tmp/tmppqi2kcfp.txt': 'tmp/tmppqi2kcfp.txt', 'tmp/tmpr04qvf8b.txt': 'tmp/tmpr04qvf8b.txt', 'tmp/tmpkc4w3_ng.txt': 'tmp/tmpkc4w3_ng.txt', 'tmp/tmps_p4jn_e.epub': 'tmp/tmps_p4jn_e.epub', 'tmp/tmp1us36_ni.epub': 'tmp/tmp1us36_ni.epub', 'tmp/tmpw0xyyd6s.txt': 'tmp/tmpw0xyyd6s.txt', 'tmp/tmp6gezu5qy.docx': 'tmp/tmp6gezu5qy.docx', 'tmp/tmp0_3t_1uq.txt': 'tmp/tmp0_3t_1uq.txt', 'tmp/tmp9hk0pxzj.docx': 'tmp/tmp9hk0pxzj.docx', 'tmp/tmp6xazsv2k.docx': 'tmp/tmp6xazsv2k.docx', 'tmp/tmpxjzndpwv.txt': 'tmp/tmpxjzndpwv.txt', 'tmp/tmp6os39ebg.epub': 'tmp/tmp6os39ebg.epub', 'tmp/tmpcfvd9w35.txt': 'tmp/tmpcfvd9w35.txt', 'tmp/tmpev89zb92.epub': 'tmp/tmpev89zb92.epub', 'tmp/tmp27vs5n2s.docx': 'tmp/tmp27vs5n2s.docx', 'tmp/tmpsg6_pghy.epub': 'tmp/tmpsg6_pghy.epub', 'tmp/tmp5j8ktx86.txt': 'tmp/tmp5j8ktx86.txt'}, 'files': [PosixPath('zzz/tmpzoho_vd7.docx'), PosixPath('zzz/tmp89zqc_jv.epub'), PosixPath('zzz/tmpmt0iwb6m.docx'), PosixPath('zzz/tmp0sbkc8r2.txt'), PosixPath('zzz/tmpocglwi1r.docx'), PosixPath('zzz/tmpe2lc8mex.epub'), PosixPath('zzz/tmpv5rwd8pz.epub'), PosixPath('zzz/tmpy7mvapwz.epub'), PosixPath('zzz/tmp2bhcq04q.epub'), PosixPath('zzz/tmpq8h90phv.docx'), PosixPath('zzz/tmp7kbo4ecl.epub'), PosixPath('zzz/tmp76e2l063.epub'), PosixPath('zzz/tmpxzo6__jy.txt'), PosixPath('zzz/tmpjmkks4l9.docx'), PosixPath('zzz/tmpg3dn3h33.txt'), PosixPath('zzz/tmp5rb23zbf.epub'), PosixPath('zzz/tmpzj9qv0_f.docx'), PosixPath('zzz/tmplzx7gu69.txt'), PosixPath('zzz/tmpl_nqcygs.docx'), PosixPath('zzz/tmpa2lw8d93.txt'), PosixPath('zzz/tmpk1wu8zaf.txt'), PosixPath('zzz/tmpxg37xiob.epub'), PosixPath('zzz/tmpoxpwctgv.docx'), PosixPath('zzz/tmpyvo9auke.docx'), PosixPath('zzz/tmpppe5qssx.epub'), PosixPath('zzz/tmpjkn4zd8j.txt'), PosixPath('zzz/tmplu9crnpu.txt'), PosixPath('zzz/tmpbkae0n_8.epub'), PosixPath('zzz/tmpf6e4h0ni.epub'), PosixPath('zzz/tmpyhxqp5nv.docx'), PosixPath('zzz/tmptmk9fpto.epub'), PosixPath('zzz/tmp2wb2r610.txt'), PosixPath('zzz/tmpzb3i8a84.docx'), PosixPath('zzz/tmppqi2kcfp.txt'), PosixPath('zzz/tmpr04qvf8b.txt'), PosixPath('zzz/tmpkc4w3_ng.txt'), PosixPath('zzz/tmps_p4jn_e.epub'), PosixPath('zzz/tmp1us36_ni.epub'), PosixPath('zzz/tmpw0xyyd6s.txt'), PosixPath('zzz/tmp6gezu5qy.docx'), PosixPath('zzz/tmp0_3t_1uq.txt'), PosixPath('zzz/tmp9hk0pxzj.docx'), PosixPath('zzz/tmp6xazsv2k.docx'), PosixPath('zzz/tmpxjzndpwv.txt'), PosixPath('zzz/tmp6os39ebg.epub'), PosixPath('zzz/tmpcfvd9w35.txt'), PosixPath('zzz/tmpev89zb92.epub'), PosixPath('zzz/tmp27vs5n2s.docx'), PosixPath('zzz/tmpsg6_pghy.epub'), PosixPath('zzz/tmp5j8ktx86.txt')], 'filename': '/tmp/tmp/zzz_archive__nknjpdp.zip'}\n"
     ]
    }
   ],
   "source": [
    "from faker import Faker\n",
    "from faker_file.providers.helpers.inner import (\n",
    "    create_inner_docx_file,\n",
    "    create_inner_epub_file,\n",
    "    create_inner_txt_file,\n",
    "    fuzzy_choice_create_inner_file,\n",
    ")\n",
    "from faker_file.providers.zip_file import ZipFileProvider\n",
    "from faker_file.storages.filesystem import FileSystemStorage\n",
    "\n",
    "FAKER = Faker()\n",
    "STORAGE = FileSystemStorage()\n",
    "\n",
    "kwargs = {\"storage\": STORAGE, \"generator\": FAKER}\n",
    "file = ZipFileProvider(FAKER).zip_file(\n",
    "    prefix=\"zzz_archive_\",\n",
    "    options={\n",
    "        \"count\": 50,\n",
    "        \"create_inner_file_func\": fuzzy_choice_create_inner_file,\n",
    "        \"create_inner_file_args\": {\n",
    "            \"func_choices\": [\n",
    "                (create_inner_docx_file, kwargs),\n",
    "                (create_inner_epub_file, kwargs),\n",
    "                (create_inner_txt_file, kwargs),\n",
    "            ],\n",
    "        },\n",
    "        \"directory\": \"zzz\",\n",
    "    },\n",
    ")\n",
    "\n",
    "print(file)\n",
    "print(file.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another way to create a ZIP file with fixed variety of different file types within\n",
    "- 3 files in the ZIP archive (1 DOCX, and 2 XML types).\n",
    "- Content is generated dynamically.\n",
    "- Filename of the archive itself is alice-looking-through-the-glass.zip.\n",
    "- Files inside the archive have fixed name (passed with basename argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/alice-looking-through-the-glass.zip\n",
      "{'inner': {'tmp/doc.docx': 'tmp/doc.docx', 'tmp/doc_metadata.xml': 'tmp/doc_metadata.xml', 'tmp/doc_isbn.xml': 'tmp/doc_isbn.xml'}, 'files': [PosixPath('doc.docx'), PosixPath('doc_metadata.xml'), PosixPath('doc_isbn.xml')], 'filename': '/tmp/tmp/alice-looking-through-the-glass.zip'}\n"
     ]
    }
   ],
   "source": [
    "from faker import Faker\n",
    "from faker_file.providers.helpers.inner import (\n",
    "    create_inner_docx_file,\n",
    "    create_inner_xml_file,\n",
    "    list_create_inner_file,\n",
    ")\n",
    "from faker_file.providers.zip_file import ZipFileProvider\n",
    "from faker_file.storages.filesystem import FileSystemStorage\n",
    "\n",
    "FAKER = Faker()\n",
    "STORAGE = FileSystemStorage()\n",
    "\n",
    "kwargs = {\"storage\": STORAGE, \"generator\": FAKER}\n",
    "file = ZipFileProvider(FAKER).zip_file(\n",
    "    basename=\"alice-looking-through-the-glass\",\n",
    "    options={\n",
    "        \"create_inner_file_func\": list_create_inner_file,\n",
    "        \"create_inner_file_args\": {\n",
    "            \"func_list\": [\n",
    "                (create_inner_docx_file, {\"basename\": \"doc\"}),\n",
    "                (create_inner_xml_file, {\"basename\": \"doc_metadata\"}),\n",
    "                (create_inner_xml_file, {\"basename\": \"doc_isbn\"}),\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "print(file)\n",
    "print(file.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using raw=True features in tests\n",
    "If you pass raw=True argument to any provider or inner function, instead of creating a file, you will get bytes back (or to be totally correct, bytes-like object BytesValue, which is basically bytes enriched with meta-data). You could then use the bytes content of the file to build a test payload as shown in the example test below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import BytesIO\n",
    "\n",
    "from django.test import TestCase\n",
    "from django.urls import reverse\n",
    "from faker import Faker\n",
    "from faker_file.providers.docx_file import DocxFileProvider\n",
    "from rest_framework.status import HTTP_201_CREATED\n",
    "from upload.models import Upload\n",
    "\n",
    "FAKER = Faker()\n",
    "FAKER.add_provider(DocxFileProvider)\n",
    "\n",
    "class UploadTestCase(TestCase):\n",
    "    \"\"\"Upload test case.\"\"\"\n",
    "\n",
    "    def test_create_docx_upload(self) -> None:\n",
    "        \"\"\"Test create an Upload.\"\"\"\n",
    "        url = reverse(\"api:upload-list\")\n",
    "\n",
    "        raw = FAKER.docx_file(raw=True)\n",
    "        test_file = BytesIO(raw)\n",
    "        test_file.name = os.path.basename(raw.data[\"filename\"])\n",
    "\n",
    "        payload = {\n",
    "            \"name\": FAKER.word(),\n",
    "            \"description\": FAKER.paragraph(),\n",
    "            \"file\": test_file,\n",
    "        }\n",
    "\n",
    "        response = self.client.post(url, payload, format=\"json\")\n",
    "\n",
    "        # Test if request is handled properly (HTTP 201)\n",
    "        self.assertEqual(response.status_code, HTTP_201_CREATED)\n",
    "\n",
    "        test_upload = Upload.objects.get(id=response.data[\"id\"])\n",
    "\n",
    "        # Test if the name is properly recorded\n",
    "        self.assertEqual(str(test_upload.name), payload[\"name\"])\n",
    "\n",
    "        # Test if file name recorded properly\n",
    "        self.assertEqual(str(test_upload.file.name), test_file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a HTML file predefined template\n",
    "If you want to generate a file in a format that is not (yet) supported, you can try to use `GenericFileProvider`. In the following example, an HTML file is generated from a template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/tmpzey9253h.html\n",
      "{'content': '<html><body><p>{{text}}</p></body></html>', 'filename': '/tmp/tmp/tmpzey9253h.html'}\n"
     ]
    }
   ],
   "source": [
    "from faker import Faker\n",
    "from faker_file.providers.generic_file import GenericFileProvider\n",
    "\n",
    "file = GenericFileProvider(Faker()).generic_file(\n",
    "    content=\"<html><body><p>{{text}}</p></body></html>\",\n",
    "    extension=\"html\",\n",
    ")\n",
    "\n",
    "print(file)\n",
    "print(file.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storages\n",
    "\n",
    "#### Example usage with `Django` (using local file system storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp/tmprtvcie4z.txt\n",
      "/home/delusionalinsanity/repos/faker-file/examples/django_example/project/media/tmp/tmprtvcie4z.txt\n",
      "\n",
      "“The Queen of Hearts, she made some tarts,\n",
      "    All on a summer day:\n",
      "The Knave of Hearts, he stole those tarts,\n",
      "    And took them quite away.”\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from django.conf import settings\n",
    "from faker_file.providers.txt_file import TxtFileProvider\n",
    "from faker_file.storages.filesystem import FileSystemStorage\n",
    "\n",
    "STORAGE = FileSystemStorage(\n",
    "    root_path=settings.MEDIA_ROOT,\n",
    "    rel_path=\"tmp\",\n",
    ")\n",
    "\n",
    "FAKER.add_provider(TxtFileProvider)\n",
    "\n",
    "file = FAKER.txt_file(content=TEXT, storage=STORAGE)\n",
    "\n",
    "print(file)\n",
    "print(file.data[\"filename\"])\n",
    "print(file.data[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example usage with AWS S3 storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO 2023-05-26 12:05:26,744 [/home/delusionalinsanity/.virtualenvs/faker-file/lib/python3.10/site-packages/botocore/credentials.py:1251] Found credentials in shared credentials file: ~/.aws/credentials\n",
      "\n",
      "INFO 2023-05-26 12:05:27,401 [/home/delusionalinsanity/.virtualenvs/faker-file/lib/python3.10/site-packages/smart_open/s3.py:901] smart_open.s3.MultipartWriter('artur-testing-1', 'tmp/sub-tmp/tmpwneu9vxt.txt'): uploading part_num: 1, 9908 bytes (total 0.000GB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub-tmp/tmpwneu9vxt.txt\n",
      "s3://artur-testing-1/tmp/sub-tmp/tmpwneu9vxt.txt\n",
      "Chance drive wear simply always. Prepare per always up me near. Turn very none newspaper sea be through. Since push six her.\n",
      "Where cultural trade of goal full follow. Ground dream central page happy write floor method.\n",
      "Ask operation tough personal word thousand little. Road be someone.\n",
      "Whatever argue growth will big nation full.\n",
      "Out situation edge. Successful energy age go us.\n",
      "Value each open parent. Less sing quality team.\n",
      "Management describe me break take ok. Join industry break where source hear especially.\n",
      "Citizen building drive history. Single technology whom environment. Successful fund many.\n",
      "Among really write action class. But area western ok pass.\n",
      "Capital believe although. Test alone old offer language. Her southern wind letter.\n",
      "Shake staff happen. Next discuss ask create better.\n",
      "Form benefit day discover. Eye person whom go knowledge doctor onto.\n",
      "Family wish project fall like your step. Lawyer bed participant final.\n",
      "Figure probably performance even imagine. Case never deal site that consider. Tough building your moment alone.\n",
      "Item edge performance field long. Group magazine product team begin both. Conference myself risk despite be.\n",
      "College spend positive glass world not. Nearly design citizen none economy break. May much mother common small.\n",
      "In per sort American fast return medical. Else everyone painting couple defense its. Off history stop visit policy remain. Forget one girl.\n",
      "Heart blood back your want impact finally. Yard whole to section there. Large evidence physical until.\n",
      "Eye television sense. Enter describe young ball like movement style person. Quite quickly hair south other.\n",
      "Old summer sort north investment. Cut question wife improve year.\n",
      "Rest past ground student. Yet garden certainly outside.\n",
      "Defense time identify economy seek. Toward I itself off miss.\n",
      "Recently either enjoy science. Protect reduce letter impact baby teach break. Nothing no land miss. Be material after end street.\n",
      "After whole economic budget capital after. Reduce our assume. Hospital seat leg life image seven watch. Throw general natural paper try material eat establish.\n",
      "Official south decision find language management. Lot myself police character approach. Training science indicate religious Congress deep join.\n",
      "Brother finish suddenly worry. Arrive defense her responsibility.\n",
      "From be how remain. Company young response mean.\n",
      "Yes within store reveal its exactly talk. Decision place cultural certain part defense.\n",
      "Her give statement get big deal. Service serious think election chair against.\n",
      "Again source explain simply medical. Between ok enough. Debate region whatever check war. Interesting of least floor control either woman.\n",
      "Majority type score our. Since miss describe Congress suddenly cold.\n",
      "Region back knowledge collection. Year number add tend.\n",
      "They travel true physical property analysis outside. Force quality send southern war.\n",
      "Fall check report month message try so dream. Child through right power protect today.\n",
      "Recognize Democrat doctor several year central option. Fact area inside defense really. Always national third new simply.\n",
      "But behavior about economy I. Training TV push black author executive. Stuff member tend audience inside.\n",
      "Side court central manage difficult high attorney response.\n",
      "Yard model in affect gas agent go. Attention himself if office amount.\n",
      "Young factor sport.\n",
      "Indicate on moment arm finally. Wind card marriage country leader senior economy kid.\n",
      "Like culture work say help subject. Range room eye. Difficult since early know station.\n",
      "See smile along only political its detail. Store record notice run.\n",
      "Couple decade civil simply this produce last hope. Today put drug police. Size the why.\n",
      "Behavior summer similar agent treatment throw future.\n",
      "Behavior guess responsibility opportunity.\n",
      "They ball officer conference total reason myself. Crime run question cell student five physical. Parent degree right beautiful rock.\n",
      "Shoulder win wife public official rather west. See foot such.\n",
      "Million send agent behavior TV reach new. Rule suddenly need vote matter seek lead.\n",
      "Garden turn me in since. Kid military stay western again war land kid. Three of change. Evidence coach send open wear old.\n",
      "Defense of school industry south voice arrive. Job hope resource care red though policy. Under family raise chance together gun pay. Contain decision take attack.\n",
      "Require after learn already. Couple state writer particular heart special article office.\n",
      "Manage education just whole put care whether. Pass few just upon pick trade usually account. Degree hold wonder continue wrong discover policy.\n",
      "Show offer laugh possible campaign such. Because situation because actually rather.\n",
      "Wrong tend fire your sell difference material. Daughter surface cultural party along parent there.\n",
      "Place rise into allow space affect. Mind matter represent decade public wind to movie.\n",
      "Capital parent reflect continue. Toward find book price able computer skin.\n",
      "Crime carry not. Final answer her heart question each set look. May evening special dog campaign.\n",
      "Sound similar camera new list. Heavy western degree room by. Will quickly set standard college crime.\n",
      "Argue middle word smile. Deep cup final federal.\n",
      "Dog discussion former hotel task find. Happy by raise big nature. Practice tough enjoy. Church big live skin.\n",
      "Amount month expect article media. While customer the themselves once before reason. Recent time buy.\n",
      "Important pick traditional. Though year sort up.\n",
      "Per around rate new new. Help message support. Any seem fall generation dinner.\n",
      "Social shake finally great enough wind again. Third vote save window their create memory power.\n",
      "Full visit accept employee fund a purpose. Recent wait manage form recently drug. Can traditional top yourself much government budget.\n",
      "Sell prevent take exactly mention. Herself Congress family maybe exactly. Or wish others require.\n",
      "Card admit cost as reduce woman develop fact. Use write personal figure. Can although network sell every operation detail.\n",
      "Move second generation I physical month camera. Mention money ability phone organization food world.\n",
      "Live image kitchen account. Trouble dream entire though resource when story which. At although song turn analysis approach stage power.\n",
      "Player me attack computer trade. Shake research reflect available. Offer economy evening race protect low course several.\n",
      "Themselves manager one doctor particular. Interest process cold. Listen national half position easy dog so pressure.\n",
      "Door should what social through. Despite sign approach heavy. Still news stuff hear miss child.\n",
      "Stand thank strong top teach reduce. Stuff this you few born production past. Account chance necessary standard yeah image his various.\n",
      "Community perform notice sound foreign. Middle end lose. Single send director word type character.\n",
      "Safe somebody reach fine do son. Together identify offer food third success. Around simply education information seek executive second hand.\n",
      "Study brother just difficult thank official discussion.\n",
      "Common senior wait long own. Just learn glass government one receive.\n",
      "Huge from late suggest for. Down discuss president just add.\n",
      "Involve vote maybe teach. Quickly economy admit.\n",
      "Suffer upon several open subject.\n",
      "Sport sometimes everybody wrong side money. Little partner personal treatment agent beautiful lay. Deep big foot prepare return.\n",
      "Peace other realize clearly this. Future none no tonight American month base. Do customer whatever debate hear week. Future reveal range player of public can.\n",
      "Food mouth inside second sound out. Doctor already guess item. Sense me nothing money us family field.\n",
      "Operation risk bad concern card. Summer sometimes now hand education relate. Begin alone third pattern apply task certain few. Letter positive suggest member score do also go.\n",
      "Detail manager soldier size best economic above. Show structure hand field.\n",
      "Despite analysis person level five baby year. Need do shoulder put design. Most must eight. Probably station poor.\n",
      "Any several will. Detail better tax. Four not pass most.\n",
      "Accept see early suggest artist. Push country else. Despite writer really move world environmental.\n",
      "Government economy wife whatever. Recognize direction successful hear fine.\n",
      "Class study mouth shoulder. Whole anything somebody knowledge visit notice rule.\n",
      "Evening church crime require. Hard piece together everyone. Admit event this TV.\n",
      "Improve huge election challenge. Give fear surface officer. Go bill health control able big range.\n",
      "Test security play. Thus agreement seek.\n",
      "Station wife fact. But include increase capital them organization significant. Face huge authority challenge.\n",
      "Interest during former. Party special door no treat stock.\n",
      "Eight generation research.\n",
      "Far can where senior establish wall moment. Despite news region finish player. Believe listen partner individual.\n",
      "Develop song PM. Goal time much development rich imagine.\n",
      "Help himself executive news character. Church pull would method degree. Build once either know organization standard short.\n",
      "Nature other something later class contain seem. Sort wrong skill model. Ready leave rather around no focus modern.\n",
      "Series read throw too happy blood knowledge instead. Result night phone machine. Against huge factor quite both walk.\n",
      "Too two strong generation chance building range. Bar put onto amount class eight kind bit.\n",
      "Around north speech even recognize morning. Perhaps community pick wind suddenly.\n",
      "Thank decision teacher agree not physical hope. Head bad stay either plan police act discuss.\n",
      "Goal full single today she however learn. Similar business only sound among.\n",
      "Too dream establish tend of evidence. See body left draw parent poor hour former.\n",
      "Economy rest with place. Music a study later process so. Stock once animal federal drug everybody free dark.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That president successful moment. Card position thing notice produce. Anyone today space sign least window bit production. Their professional activity media hard sometimes financial.\n"
     ]
    }
   ],
   "source": [
    "from faker_file.storages.aws_s3 import AWSS3Storage\n",
    "\n",
    "S3_STORAGE = AWSS3Storage(\n",
    "    bucket_name=\"artur-testing-1\",\n",
    "    root_path=\"tmp\",  # Optional\n",
    "    rel_path=\"sub-tmp\",  # Optional\n",
    "    # Credentials are optional too. If your AWS credentials are properly\n",
    "    # set in the ~/.aws/credentials, you don't need to send them\n",
    "    # explicitly.\n",
    "    # credentials={\n",
    "    #     \"key_id\": \"YOUR KEY ID\",\n",
    "    #     \"key_secret\": \"YOUR KEY SECRET\"\n",
    "    # },\n",
    ")\n",
    "\n",
    "file = FAKER.txt_file(storage=S3_STORAGE)\n",
    "\n",
    "print(file)\n",
    "print(file.data[\"filename\"])\n",
    "print(file.data[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augment existing files\n",
    "If you think `Faker` generated data doesn't make sense for you and you want\n",
    "your files to look like a collection of 100 files you already have, you could\n",
    "use augmentation features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/me/Documents/faker_file_source/'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 7\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfaker_file\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mproviders\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01maugment_file_from_dir\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m      2\u001B[0m     AugmentFileFromDirProvider,\n\u001B[1;32m      3\u001B[0m )\n\u001B[1;32m      5\u001B[0m FAKER\u001B[38;5;241m.\u001B[39madd_provider(AugmentFileFromDirProvider)\n\u001B[0;32m----> 7\u001B[0m file \u001B[38;5;241m=\u001B[39m \u001B[43mFAKER\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maugment_file_from_dir\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43msource_dir_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/home/me/Documents/faker_file_source/\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwrap_chars_after\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m120\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28mprint\u001B[39m(file)\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(file\u001B[38;5;241m.\u001B[39mdata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfilename\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n",
      "File \u001B[0;32m~/repos/faker-file/src/faker_file/providers/augment_file_from_dir/__init__.py:187\u001B[0m, in \u001B[0;36mAugmentFileFromDirProvider.augment_file_from_dir\u001B[0;34m(self, source_dir_path, extensions, storage, basename, prefix, wrap_chars_after, text_extractor_cls, text_extractor_kwargs, text_augmenter_cls, text_augmenter_kwargs, raw, **kwargs)\u001B[0m\n\u001B[1;32m    182\u001B[0m     extensions \u001B[38;5;241m=\u001B[39m EXTENSIONS\n\u001B[1;32m    184\u001B[0m \u001B[38;5;66;03m# Specific\u001B[39;00m\n\u001B[1;32m    185\u001B[0m source_file_choices \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    186\u001B[0m     os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(source_dir_path, _f)\n\u001B[0;32m--> 187\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m _f \u001B[38;5;129;01min\u001B[39;00m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlistdir\u001B[49m\u001B[43m(\u001B[49m\u001B[43msource_dir_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    188\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    189\u001B[0m         os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(source_dir_path, _f))\n\u001B[1;32m    190\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39msplitext(_f)[\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m1\u001B[39m:] \u001B[38;5;129;01min\u001B[39;00m extensions\n\u001B[1;32m    191\u001B[0m     )\n\u001B[1;32m    192\u001B[0m ]\n\u001B[1;32m    193\u001B[0m source_file_path \u001B[38;5;241m=\u001B[39m choice(source_file_choices)\n\u001B[1;32m    194\u001B[0m source_file \u001B[38;5;241m=\u001B[39m Path(source_file_path)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/home/me/Documents/faker_file_source/'"
     ]
    }
   ],
   "source": [
    "from faker_file.providers.augment_file_from_dir import (\n",
    "    AugmentFileFromDirProvider,\n",
    ")\n",
    "\n",
    "FAKER.add_provider(AugmentFileFromDirProvider)\n",
    "\n",
    "file = FAKER.augment_file_from_dir(\n",
    "    source_dir_path=\"/home/me/Documents/faker_file_source/\",\n",
    "    wrap_chars_after=120,\n",
    ")\n",
    "\n",
    "print(file)\n",
    "print(file.data[\"filename\"])\n",
    "print(file.data[\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLI\n",
    "Even if you're not using automated testing, but still want to quickly generate a file with fake content, you could use faker-file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated bash completion file: /home/delusionalinsanity/faker_file_completion.sh\r\n"
     ]
    }
   ],
   "source": [
    "  !faker-file generate-completion\n",
    "  !source ~/faker_file_completion.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate an MP3 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated mp3_file file (1 of 1): /tmp/tmp/my_file_hgydrlfu.mp3\r\n"
     ]
    }
   ],
   "source": [
    "!faker-file mp3_file --prefix=my_file_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate 10 DOCX files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated docx_file file (1 of 10): /tmp/tmp/my_file_kmx9jj5m.docx\n",
      "Generated docx_file file (2 of 10): /tmp/tmp/my_file_i788xvlk.docx\n",
      "Generated docx_file file (3 of 10): /tmp/tmp/my_file_t2z0p1ox.docx\n",
      "Generated docx_file file (4 of 10): /tmp/tmp/my_file_2ldcku1a.docx\n",
      "Generated docx_file file (5 of 10): /tmp/tmp/my_file_fzfxyw_0.docx\n",
      "Generated docx_file file (6 of 10): /tmp/tmp/my_file_owbrwouu.docx\n",
      "Generated docx_file file (7 of 10): /tmp/tmp/my_file_mrqa_uj6.docx\n",
      "Generated docx_file file (8 of 10): /tmp/tmp/my_file_nscn7yae.docx\n",
      "Generated docx_file file (9 of 10): /tmp/tmp/my_file_4r_4rhu8.docx\n",
      "Generated docx_file file (10 of 10): /tmp/tmp/my_file_gzm4c00i.docx\n"
     ]
    }
   ],
   "source": [
    "!faker-file docx_file --nb_files 10 --prefix=my_file_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Kernel",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
